# Consensus: Evaluate this scoring rubric for note-taking systems. The proposed criteria are:

1. Capture friction (0-10) - can you land in today's note instantly?
2. Retrieval power (0-10) - keyword + semantic search
3. Surfacing (0-10) - tasks, snoozes, time-bound resurfacing
4. Modularity/permissions (0-10) - AI can touch some folders, not all
5. Maintainability (0-10) - upgrades don't break you
6. Mobile capture (0-10) - voice → text → filed

Questions:
1. Are these the right criteria? What's missing?
2. How should the weights be distributed? (Should any be weighted higher?)
3. What scoring methodology would be most robust?

**Confidence:** HIGH
**Agreement:** MAJORITY

The rubric is a strong baseline but requires three specific adjustments to be robust: 1) Add 'Connection Discovery' and 'Cognitive Load' as criteria, 2) Weight 'Capture' and 'Retrieval' as the dominant factors (combined ~50-60%), and 3) Treat 'Modularity/Permissions' as a binary pass/fail gate rather than a scored scale.

## Summary

Experts agree that while the foundation is solid, the system misses the 'serendipity' of connecting notes and the 'cognitive load' impact of UI clutter. There is consensus that Capture and Retrieval are the critical survival metrics for any note-taking system and must be weighted highest. A significant methodological improvement is to treat security (Modularity) as a prerequisite gate, not a variable score.